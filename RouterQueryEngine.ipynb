{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Router Query Engine"
      ],
      "metadata": {
        "id": "D1Y3gSVdHdzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will be using a router query engine, which will choose one of multiple candidate query engines to execute user query.\n",
        "\n",
        "[Documentation](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/RouterQueryEngine.html)"
      ],
      "metadata": {
        "id": "fuAwUKiCCkze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "vlLzNfQrC54i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOZ6iXTJHlQ5",
        "outputId": "3c9ec6a5-fe06-4a67-e36e-1fee54afc4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.8.53.post3-py3-none-any.whl (794 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.6/794.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.22)\n",
            "Collecting aiostream<0.6.0,>=0.5.2 (from llama-index)\n",
            "  Downloading aiostream-0.5.2-py3-none-any.whl (39 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from llama-index)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Collecting langchain>=0.0.303 (from llama-index)\n",
            "  Downloading langchain-0.0.325-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
            "Collecting openai>=0.26.4 (from llama-index)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.5.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting urllib3<2 (from llama-index)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->llama-index)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (4.0.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain>=0.0.303->llama-index)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain>=0.0.303->llama-index)\n",
            "  Downloading langsmith-0.0.53-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.1.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->llama-index)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->llama-index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.303->llama-index) (2023.7.22)\n",
            "Installing collected packages: urllib3, mypy-extensions, marshmallow, jsonpointer, deprecated, aiostream, typing-inspect, jsonpatch, tiktoken, openai, langsmith, dataclasses-json, langchain, llama-index\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiostream-0.5.2 dataclasses-json-0.5.14 deprecated-1.2.14 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.325 langsmith-0.0.53 llama-index-0.8.53.post3 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.28.1 tiktoken-0.5.1 typing-inspect-0.9.0 urllib3-1.26.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: This is ONLY necessary in jupyter notebook.\n",
        "# Details: Jupyter runs an event-loop behind the scenes.\n",
        "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
        "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "R_Ttz5WDI8M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGEOh3sdFuQ-"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "# Set up the root logger\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)  # Set logger level to INFO\n",
        "\n",
        "# Clear out any existing handlers\n",
        "logger.handlers = []\n",
        "\n",
        "# Set up the StreamHandler to output to sys.stdout (Colab's output)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setLevel(logging.INFO)  # Set handler level to INFO\n",
        "\n",
        "# Add the handler to the logger\n",
        "logger.addHandler(handler)\n",
        "\n",
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    SummaryIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        ")\n",
        "\n",
        "import openai\n",
        "openai.api_key = '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'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Data"
      ],
      "metadata": {
        "id": "igDWYnajC_tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0ePCxRKDIXl",
        "outputId": "0a06faf1-4c14-42f1-ca54-770024f25416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-28 06:54:23--  https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
            "\n",
            "\r          data/paul   0%[                    ]       0  --.-KB/s               \rdata/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-10-28 06:54:23 (2.86 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "l0Cp38vaDGbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
      ],
      "metadata": {
        "id": "7UuQpLoRD1Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define List Index and Vector Index over Same Data"
      ],
      "metadata": {
        "id": "e4LNrmhrK4xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_index = SummaryIndex.from_documents(documents)\n",
        "vector_index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "B9C2Gm1UF8r3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a62352-8e11-487b-c128-78226150339f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Query Engines and Set Metadata"
      ],
      "metadata": {
        "id": "voktqsQNLApj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "uLMKbMAUGA9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.tools.query_engine import QueryEngineTool\n",
        "\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=\"Useful for summarization questions related to Paul Graham eassy on What I Worked On.\",\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=\"Useful for retrieving specific context from Paul Graham essay on What I Worked On.\",\n",
        ")"
      ],
      "metadata": {
        "id": "fg4aOwPaGNxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Router Query Engine\n",
        "\n",
        "There are several selectors available, each with some distinct attributes.\n",
        "\n",
        "The LLM selectors use the LLM to output a JSON that is parsed, and the corresponding indexes are queried.\n",
        "\n",
        "The Pydantic selectors (currently only supported by gpt-4-0613 and gpt-3.5-turbo-0613 (the default)) use the OpenAI Function Call API to produce pydantic selection objects, rather than parsing raw JSON.\n",
        "\n",
        "For each type of selector, there is also the option to select 1 index to route to, or multiple."
      ],
      "metadata": {
        "id": "F0zHAQS_LF3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PydanticSingleSelector"
      ],
      "metadata": {
        "id": "jTncjEj2LH88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.selectors.llm_selectors import LLMSingleSelector\n",
        "from llama_index.selectors.pydantic_selectors import (\n",
        "    PydanticSingleSelector,\n",
        ")\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=PydanticSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "GGjl2y5QGRcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")"
      ],
      "metadata": {
        "id": "0hywia-DGTx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba3570f-b762-441a-f72c-cecc555684c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting query engine 0: This choice is specifically mentioned as useful for summarization questions..\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1163 request_id=e05e7bb51af46d9a7448aa88f6db49f9 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2408 request_id=f947948968055c2ecd550024945d9e2d response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2822 request_id=a29845bf105af2a01cffdb6663193a69 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3755 request_id=f1f69879f96a0900818da7b462e5db90 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3817 request_id=785f434f8942b7634e7540ccb7126be7 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4287 request_id=41303f2e223c83b936200dbe89baa191 response_code=200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "sriu-0zoLR7q",
        "outputId": "c5ba929c-b2b2-4938-b03b-c38311f4d37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"font-size:20px\">The document provides a personal account of the author's experiences and interests in writing, programming, and art. It discusses their early experiences in writing and programming, their fascination with artificial intelligence, and their decision to focus on Lisp programming. The document also shares their experiences studying art and their fascination with still life painting. It highlights their transition into the world of technology and entrepreneurship, their interest in the World Wide Web, and their creation and development of a software company called Viaweb. The document also discusses the author's realization of the power of publishing on the web, their involvement in various projects, including starting an investment firm called Y Combinator, and their personal experiences and reflections on family and making meaningful contributions in life. It concludes with the author's thoughts on choosing projects and their decision to write the document itself.</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMSingleSelector"
      ],
      "metadata": {
        "id": "5zkEOzNYLUQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "TbnqXGG-Ga3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")"
      ],
      "metadata": {
        "id": "d_xrq9gYGdf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc85357-c716-4450-f633-94f8c4d5e532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting query engine 0: Both choices are identical, so either choice can be selected..\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1209 request_id=1affb0171ea3b0881ba09daf574d448c response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2603 request_id=ec2ba1aa8a712a5612a851f1e634b98d response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2765 request_id=4a386ac81ff25fe67d9271433f8ebafa response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3382 request_id=93e6a7c9d95a719b5c331a158fab44e0 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3987 request_id=63de7fd56c1f40f63e507a65ca1bfef8 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3475 request_id=56314fac79cf00c19dbbd9bdf43bd8b0 response_code=200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "ksusZylqG_Nb",
        "outputId": "ce15745f-eb8a-4e1a-cf7b-6ffdc12a5bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"font-size:20px\">The document provides a personal account of the author's experiences and interests in writing, programming, and art. It discusses their early experiences in writing and programming, their fascination with artificial intelligence, and their transition to Lisp programming. The author also shares their decision to pursue art and their experiences at art school. The document highlights the author's involvement in starting a company called Viaweb, their challenges in getting it off the ground, and their realization of the potential of the World Wide Web. It also discusses the author's journey in building the infrastructure of the web, their involvement in various projects, and the founding and growth of Y Combinator. The document concludes with the author's reflection on their career choices and their decision to write the document itself.</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What did Paul Graham do after RICS?\")"
      ],
      "metadata": {
        "id": "IEfQDl8LGd8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200e03b0-82f6-46ba-b336-05788f3d5d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting query engine 1: The question is asking about what Paul Graham did after RICS, and both choices mention the essay on What I Worked On, so either choice could be relevant. However, since the question is specifically asking about what Paul Graham did, choice 2 seems more relevant as it directly mentions the essay on What I Worked On..\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=390 request_id=697dec41c3f0c1e30ec17ad51c07eb75 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1143 request_id=377df2450caaaf4e9c7ba02227f9013e response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2354 request_id=c7871a1eab1c83bac5a0164e7325482b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2576 request_id=bb0ecdb2bdf7a465b446f86eb882a087 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2683 request_id=7c2df7e388eb2e0bd3a2ae8767f3a988 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4094 request_id=6b31c25ab50c6726de71d8f9e36ce38f response_code=200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "S1DXw_KxHApd",
        "outputId": "33f40858-2148-4020-b648-1d8b59be94a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"font-size:20px\">After RICS, Paul Graham pursued various paths. He took art classes at Harvard and applied to art schools, eventually being accepted into the BFA program at RISD. He then moved to New York City and became a painter while also working as a de facto studio assistant for Idelle Weber. However, he later decided to explore other opportunities and co-founded a company called Viaweb, which developed software for online stores. This venture received seed funding and eventually led to the creation of Y Combinator, a startup accelerator. Additionally, Paul Graham started a new company called Aspra but later shifted his focus to building an open-source project and working on a new dialect of Lisp called Arc. He also began publishing essays online, which became a significant turning point for him.</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vCZzlGNRHBIi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}